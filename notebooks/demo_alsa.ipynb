{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# A-LSA Demonstration Notebook\n",
    "\n",
    "**Adaptive Latent Semantic Analysis for Binary Text Classification**\n",
    "\n",
    "Author: Isaac Touza  \n",
    "Institution: Université de Maroua, Cameroun  \n",
    "Date: January 2026\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates the complete A-LSA workflow across **three diverse datasets**:\n",
    "\n",
    "1. **20 Newsgroups** - Topic classification (comp.graphics vs rec.sport.hockey)\n",
    "2. **IMDb Movie Reviews** - Sentiment analysis (positive vs negative)\n",
    "3. **SMS Spam Collection** - Spam detection (spam vs ham)\n",
    "\n",
    "For each dataset, we will:\n",
    "- Load and preprocess the data\n",
    "- Train A-LSA and baseline models\n",
    "- Evaluate and compare performance\n",
    "- Visualize results and characteristic terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.alsa import AdaptiveLSA\n",
    "from src.baselines import get_baseline_models\n",
    "from src.evaluation import evaluate_model, compare_models\n",
    "from src.visualization import (\n",
    "    plot_tsne_visualization,\n",
    "    plot_characteristic_terms,\n",
    "    plot_performance_comparison\n",
    ")\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Configuration\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "N_COMPONENTS = 100\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-loaders",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_20newsgroups_data():\n",
    "    \"\"\"Load 20 Newsgroups dataset.\"\"\"\n",
    "    categories = ('comp.graphics', 'rec.sport.hockey')\n",
    "    \n",
    "    data = fetch_20newsgroups(\n",
    "        subset='all',\n",
    "        categories=categories,\n",
    "        remove=('headers', 'footers', 'quotes'),\n",
    "        shuffle=True,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    return data.data, data.target, categories\n",
    "\n",
    "\n",
    "def load_imdb_data(data_dir='../data/imdb', max_samples=10000):\n",
    "    \"\"\"Load IMDb Movie Reviews dataset.\"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    if not os.path.exists(os.path.join(data_dir, 'train', 'pos')):\n",
    "        print(f\"⚠ IMDb dataset not found at {data_dir}\")\n",
    "        print(\"  Download from: https://ai.stanford.edu/~amaas/data/sentiment/\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Load data\n",
    "    for split in ['train', 'test']:\n",
    "        for label_name, label_value in [('pos', 1), ('neg', 0)]:\n",
    "            dir_path = os.path.join(data_dir, split, label_name)\n",
    "            \n",
    "            if not os.path.exists(dir_path):\n",
    "                continue\n",
    "            \n",
    "            for filename in os.listdir(dir_path):\n",
    "                if filename.endswith('.txt'):\n",
    "                    with open(os.path.join(dir_path, filename), 'r', encoding='utf-8') as f:\n",
    "                        texts.append(f.read())\n",
    "                        labels.append(label_value)\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Subsample for efficiency\n",
    "    if len(texts) > max_samples:\n",
    "        indices = np.random.RandomState(RANDOM_STATE).choice(\n",
    "            len(texts), max_samples, replace=False\n",
    "        )\n",
    "        texts = [texts[i] for i in indices]\n",
    "        labels = labels[indices]\n",
    "    \n",
    "    return texts, labels, ('Negative', 'Positive')\n",
    "\n",
    "\n",
    "def load_sms_spam_data(data_path='../data/sms_spam/SMSSpamCollection'):\n",
    "    \"\"\"Load SMS Spam Collection dataset.\"\"\"\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"⚠ SMS Spam dataset not found at {data_path}\")\n",
    "        print(\"  Download from: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\")\n",
    "        return None, None, None\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t', 1)\n",
    "            if len(parts) == 2:\n",
    "                label, text = parts\n",
    "                texts.append(text)\n",
    "                labels.append(1 if label == 'spam' else 0)\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return texts, labels, ('Ham', 'Spam')\n",
    "\n",
    "\n",
    "print(\"✓ Dataset loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: 20 Newsgroups Dataset\n",
    "\n",
    "Topic classification between comp.graphics and rec.sport.hockey\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ng-load",
   "metadata": {},
   "source": [
    "## 3.1 Load 20 Newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-ng",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading 20 Newsgroups dataset...\")\n",
    "X_ng, y_ng, categories_ng = load_20newsgroups_data()\n",
    "\n",
    "print(f\"✓ Loaded {len(X_ng)} documents\")\n",
    "print(f\"  - {categories_ng[0]}: {np.sum(y_ng == 0)} documents ({100*np.mean(y_ng == 0):.1f}%)\")\n",
    "print(f\"  - {categories_ng[1]}: {np.sum(y_ng == 1)} documents ({100*np.mean(y_ng == 1):.1f}%)\")\n",
    "\n",
    "# Split data\n",
    "X_ng_train, X_ng_test, y_ng_train, y_ng_test = train_test_split(\n",
    "    X_ng, y_ng, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_ng\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining: {len(X_ng_train)} | Test: {len(X_ng_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ng-train",
   "metadata": {},
   "source": [
    "## 3.2 Train Models on 20 Newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-ng",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "print(\"Initializing models...\")\n",
    "models_ng = {}\n",
    "models_ng['A-LSA'] = AdaptiveLSA(n_components=N_COMPONENTS, random_state=RANDOM_STATE)\n",
    "models_ng.update(get_baseline_models(n_components=N_COMPONENTS, random_state=RANDOM_STATE))\n",
    "\n",
    "# Train and evaluate\n",
    "print(\"\\nTraining and evaluating models...\")\n",
    "results_ng = compare_models(\n",
    "    models=models_ng,\n",
    "    X_train=X_ng_train,\n",
    "    y_train=y_ng_train,\n",
    "    X_test=X_ng_test,\n",
    "    y_test=y_ng_test,\n",
    "    n_cv_folds=5,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "results_ng['Dataset'] = '20 Newsgroups'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"20 NEWSGROUPS RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(results_ng[['Model', 'Test F1 (macro)', 'Test Accuracy']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ng-viz",
   "metadata": {},
   "source": [
    "## 3.3 Visualize 20 Newsgroups Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-ng",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characteristic terms\n",
    "alsa_ng = models_ng['A-LSA']\n",
    "char_terms_ng = alsa_ng.get_characteristic_terms(n_terms=10)\n",
    "\n",
    "plot_characteristic_terms(\n",
    "    terms_pos=char_terms_ng['positive'],\n",
    "    terms_neg=char_terms_ng['negative'],\n",
    "    class_names=categories_ng,\n",
    "    n_terms=10\n",
    ")\n",
    "\n",
    "print(f\"\\nTop {categories_ng[1]} terms:\", [t for t, _ in char_terms_ng['positive'][:5]])\n",
    "print(f\"Top {categories_ng[0]} terms:\", [t for t, _ in char_terms_ng['negative'][:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tsne-ng",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization\n",
    "z_pos_ng, z_neg_ng = alsa_ng.get_latent_projections(X_ng_test)\n",
    "\n",
    "plot_tsne_visualization(\n",
    "    z_pos=z_pos_ng,\n",
    "    z_neg=z_neg_ng,\n",
    "    y_true=y_ng_test,\n",
    "    class_names=categories_ng,\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: IMDb Movie Reviews\n",
    "\n",
    "Sentiment analysis (positive vs negative reviews)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imdb-load",
   "metadata": {},
   "source": [
    "## 4.1 Load IMDb Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-imdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading IMDb dataset...\")\n",
    "X_imdb, y_imdb, categories_imdb = load_imdb_data()\n",
    "\n",
    "if X_imdb is not None:\n",
    "    print(f\"✓ Loaded {len(X_imdb)} reviews\")\n",
    "    print(f\"  - {categories_imdb[0]}: {np.sum(y_imdb == 0)} reviews ({100*np.mean(y_imdb == 0):.1f}%)\")\n",
    "    print(f\"  - {categories_imdb[1]}: {np.sum(y_imdb == 1)} reviews ({100*np.mean(y_imdb == 1):.1f}%)\")\n",
    "    \n",
    "    # Split data\n",
    "    X_imdb_train, X_imdb_test, y_imdb_train, y_imdb_test = train_test_split(\n",
    "        X_imdb, y_imdb, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_imdb\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining: {len(X_imdb_train)} | Test: {len(X_imdb_test)}\")\n",
    "else:\n",
    "    print(\"⚠ Skipping IMDb dataset (not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imdb-train",
   "metadata": {},
   "source": [
    "## 4.2 Train Models on IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-imdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_imdb is not None:\n",
    "    # Initialize models\n",
    "    print(\"Initializing models...\")\n",
    "    models_imdb = {}\n",
    "    models_imdb['A-LSA'] = AdaptiveLSA(n_components=N_COMPONENTS, random_state=RANDOM_STATE)\n",
    "    models_imdb.update(get_baseline_models(n_components=N_COMPONENTS, random_state=RANDOM_STATE))\n",
    "    \n",
    "    # Train and evaluate\n",
    "    print(\"\\nTraining and evaluating models...\")\n",
    "    results_imdb = compare_models(\n",
    "        models=models_imdb,\n",
    "        X_train=X_imdb_train,\n",
    "        y_train=y_imdb_train,\n",
    "        X_test=X_imdb_test,\n",
    "        y_test=y_imdb_test,\n",
    "        n_cv_folds=5,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    results_imdb['Dataset'] = 'IMDb'\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"IMDB RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_imdb[['Model', 'Test F1 (macro)', 'Test Accuracy']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imdb-viz",
   "metadata": {},
   "source": [
    "## 4.3 Visualize IMDb Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-imdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_imdb is not None:\n",
    "    # Characteristic terms\n",
    "    alsa_imdb = models_imdb['A-LSA']\n",
    "    char_terms_imdb = alsa_imdb.get_characteristic_terms(n_terms=10)\n",
    "    \n",
    "    plot_characteristic_terms(\n",
    "        terms_pos=char_terms_imdb['positive'],\n",
    "        terms_neg=char_terms_imdb['negative'],\n",
    "        class_names=categories_imdb,\n",
    "        n_terms=10\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTop {categories_imdb[1]} sentiment terms:\", [t for t, _ in char_terms_imdb['positive'][:5]])\n",
    "    print(f\"Top {categories_imdb[0]} sentiment terms:\", [t for t, _ in char_terms_imdb['negative'][:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: SMS Spam Collection\n",
    "\n",
    "Spam detection (spam vs ham messages)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sms-load",
   "metadata": {},
   "source": [
    "## 5.1 Load SMS Spam Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-sms",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading SMS Spam dataset...\")\n",
    "X_sms, y_sms, categories_sms = load_sms_spam_data()\n",
    "\n",
    "if X_sms is not None:\n",
    "    print(f\"✓ Loaded {len(X_sms)} messages\")\n",
    "    print(f\"  - {categories_sms[0]}: {np.sum(y_sms == 0)} messages ({100*np.mean(y_sms == 0):.1f}%)\")\n",
    "    print(f\"  - {categories_sms[1]}: {np.sum(y_sms == 1)} messages ({100*np.mean(y_sms == 1):.1f}%)\")\n",
    "    \n",
    "    # Split data\n",
    "    X_sms_train, X_sms_test, y_sms_train, y_sms_test = train_test_split(\n",
    "        X_sms, y_sms, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_sms\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining: {len(X_sms_train)} | Test: {len(X_sms_test)}\")\n",
    "else:\n",
    "    print(\"⚠ Skipping SMS Spam dataset (not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sms-train",
   "metadata": {},
   "source": [
    "## 5.2 Train Models on SMS Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-sms",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_sms is not None:\n",
    "    # Initialize models\n",
    "    print(\"Initializing models...\")\n",
    "    models_sms = {}\n",
    "    models_sms['A-LSA'] = AdaptiveLSA(n_components=N_COMPONENTS, random_state=RANDOM_STATE)\n",
    "    models_sms.update(get_baseline_models(n_components=N_COMPONENTS, random_state=RANDOM_STATE))\n",
    "    \n",
    "    # Train and evaluate\n",
    "    print(\"\\nTraining and evaluating models...\")\n",
    "    results_sms = compare_models(\n",
    "        models=models_sms,\n",
    "        X_train=X_sms_train,\n",
    "        y_train=y_sms_train,\n",
    "        X_test=X_sms_test,\n",
    "        y_test=y_sms_test,\n",
    "        n_cv_folds=5,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    results_sms['Dataset'] = 'SMS Spam'\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SMS SPAM RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_sms[['Model', 'Test F1 (macro)', 'Test Accuracy']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sms-viz",
   "metadata": {},
   "source": [
    "## 5.3 Visualize SMS Spam Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-sms",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_sms is not None:\n",
    "    # Characteristic terms\n",
    "    alsa_sms = models_sms['A-LSA']\n",
    "    char_terms_sms = alsa_sms.get_characteristic_terms(n_terms=10)\n",
    "    \n",
    "    plot_characteristic_terms(\n",
    "        terms_pos=char_terms_sms['positive'],\n",
    "        terms_neg=char_terms_sms['negative'],\n",
    "        class_names=categories_sms,\n",
    "        n_terms=10\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTop {categories_sms[1]} terms:\", [t for t, _ in char_terms_sms['positive'][:5]])\n",
    "    print(f\"Top {categories_sms[0]} terms:\", [t for t, _ in char_terms_sms['negative'][:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Cross-Dataset Comparison\n",
    "\n",
    "Compare A-LSA performance across all three datasets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "## 6. Compare Results Across Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = [results_ng]\n",
    "\n",
    "if X_imdb is not None:\n",
    "    all_results.append(results_imdb)\n",
    "    \n",
    "if X_sms is not None:\n",
    "    all_results.append(results_sms)\n",
    "\n",
    "combined_results = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CROSS-DATASET COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(combined_results[['Dataset', 'Model', 'Test F1 (macro)', 'Test Accuracy']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize A-LSA performance across datasets\n",
    "alsa_results = combined_results[combined_results['Model'] == 'A-LSA'].copy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# F1 scores\n",
    "axes[0].bar(alsa_results['Dataset'], alsa_results['Test F1 (macro)'], color='steelblue', alpha=0.7)\n",
    "axes[0].set_ylabel('F1 Score (macro)', fontweight='bold')\n",
    "axes[0].set_title('A-LSA F1 Performance Across Datasets', fontweight='bold')\n",
    "axes[0].set_ylim([0.7, 1.0])\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Accuracy\n",
    "axes[1].bar(alsa_results['Dataset'], alsa_results['Test Accuracy'], color='coral', alpha=0.7)\n",
    "axes[1].set_ylabel('Accuracy', fontweight='bold')\n",
    "axes[1].set_title('A-LSA Accuracy Across Datasets', fontweight='bold')\n",
    "axes[1].set_ylim([0.7, 1.0])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated A-LSA across three diverse datasets:\n",
    "\n",
    "### 20 Newsgroups\n",
    "- ✓ Topic classification task\n",
    "- ✓ Balanced dataset\n",
    "- ✓ Medium-length documents\n",
    "\n",
    "### IMDb Movie Reviews\n",
    "- ✓ Sentiment analysis task\n",
    "- ✓ Balanced dataset\n",
    "- ✓ Long documents (~230 words)\n",
    "\n",
    "### SMS Spam Collection\n",
    "- ✓ Spam detection task\n",
    "- ✓ Imbalanced dataset (13% spam)\n",
    "- ✓ Very short texts (~80 characters)\n",
    "\n",
    "**Key Findings:**\n",
    "- A-LSA achieves competitive or superior performance across all datasets\n",
    "- Dual latent spaces effectively capture class-specific semantics\n",
    "- Method is robust to varying text lengths and class imbalance\n",
    "- Characteristic terms provide interpretable insights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
