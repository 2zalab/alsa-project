# A-LSA: Adaptive Latent Semantic Analysis

**Binary Text Classification via Dual Latent Spaces**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **Author:** Isaac Touza
> **Institution:** UniversitÃ© de Maroua, Cameroun
> **Version:** 1.0
> **Date:** January 2026

---

## ğŸ“š Overview

**Adaptive Latent Semantic Analysis (A-LSA)** is a novel approach to binary text classification that constructs **separate latent semantic spaces** for each class, rather than a single unified space as in classical LSA.

### Key Innovation

Instead of building one latent space for all classes (classical LSA), A-LSA:
1. **Partitions the corpus** by class (D+ and D-)
2. **Constructs class-specific latent spaces** using SVD
3. **Classifies documents** based on differential semantic distance to each space

This enables A-LSA to capture **class-specific semantic structures** and achieve competitive performance with significantly lower computational cost than deep learning methods.

---

## ğŸ¯ Core Algorithm

### Training Phase

1. **Preprocess** all documents â†’ TF-IDF representation
2. **Partition** corpus into D+ (positive) and D- (negative)
3. **Build matrices** X+ and X- (TF-IDF for each class)
4. **Apply SVD** to each matrix:
   - X+ â‰ˆ U+ Î£+ V+áµ€
   - X- â‰ˆ U- Î£- V-áµ€
5. **Compute threshold** Î¸ as weighted midpoint of class mean distances:
   - Î¸ = (Î¼+ Ã— N+ + Î¼- Ã— N-) / (N+ + N-)
   - where Î¼+ and Î¼- are mean differential distances for each class

### Classification Phase

For a new document d:

1. **Represent** as TF-IDF vector x_d
2. **Project** into both latent spaces:
   - z+ = Î£+â»Â¹ U+áµ€ x_d
   - z- = Î£-â»Â¹ U-áµ€ x_d
3. **Compute energies**:
   - E+ = ||z+||Â²
   - E- = ||z-||Â²
4. **Calculate differential distance**: Î”_sem = E- - E+
5. **Decide**:
   - If Î”_sem < Î¸ â†’ **positive class**
   - If Î”_sem â‰¥ Î¸ â†’ **negative class**

---

## ğŸ“Š Datasets

The implementation is evaluated on three benchmark datasets:

| Dataset | Size | Classes | Balance | Domain | Challenge |
|---------|------|---------|---------|--------|-----------|
| **SMS Spam** | 5,574 | spam/ham | 13.4% / 86.6% | Short messages | Imbalance |
| **IMDb Reviews** | 50,000 | pos/neg | 50% / 50% | Movie reviews | Long texts |
| **20 Newsgroups** | ~2,000 | comp.graphics / rec.sport.hockey | ~50% / 50% | Forum posts | Domain separation |

### Dataset Links

- **SMS Spam Collection**: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection
- **IMDb Reviews**: https://ai.stanford.edu/~amaas/data/sentiment/
- **20 Newsgroups**: http://qwone.com/~jason/20Newsgroups/

---

## ğŸš€ Installation

### Requirements

- Python 3.8+
- pip

### Install Dependencies

```bash
# Clone the repository
git clone https://github.com/yourusername/alsa-project.git
cd alsa-project

# Install required packages
pip install -r requirements.txt

# Download NLTK data
python -c "import nltk; nltk.download('stopwords')"

# (Optional) Install in development mode
pip install -e .
```

---

## ğŸ’» Usage

### Quick Start

```python
from src.alsa import AdaptiveLSA

# Initialize model
model = AdaptiveLSA(n_components=100, random_state=42)

# Train
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Get probabilities
y_proba = model.predict_proba(X_test)
```

### Running Experiments

```bash
# SMS Spam experiment
python experiments/run_sms_spam.py

# IMDb experiment
python experiments/run_imdb.py

# 20 Newsgroups experiment
python experiments/run_newsgroups.py

# Sensitivity analysis
python experiments/sensitivity_analysis.py
```

### Command-Line Interface

If installed via `pip install -e .`:

```bash
# Run experiments
alsa-sms           # SMS Spam dataset
alsa-imdb          # IMDb dataset
alsa-newsgroups    # 20 Newsgroups dataset
alsa-sensitivity   # Sensitivity analysis
```

---

## ğŸ“ Project Structure

```
alsa-project/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ alsa.py                  # Core A-LSA implementation
â”‚   â”œâ”€â”€ preprocessing.py         # Text preprocessing pipeline
â”‚   â”œâ”€â”€ baselines.py             # Baseline models
â”‚   â”œâ”€â”€ evaluation.py            # Evaluation metrics
â”‚   â””â”€â”€ visualization.py         # Plotting functions
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ run_sms_spam.py          # SMS Spam experiment
â”‚   â”œâ”€â”€ run_imdb.py              # IMDb experiment
â”‚   â”œâ”€â”€ run_newsgroups.py        # 20 Newsgroups experiment
â”‚   â””â”€â”€ sensitivity_analysis.py  # Sensitivity analyses
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sms_spam/                # SMS Spam dataset
â”‚   â”œâ”€â”€ imdb/                    # IMDb dataset
â”‚   â””â”€â”€ 20newsgroups/            # 20 Newsgroups dataset
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ tables/                  # Result tables (CSV, Markdown)
â”‚   â””â”€â”€ figures/                 # Visualizations (PNG, PDF)
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ demo_alsa.ipynb          # Demonstration notebook
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

---

## ğŸ“ˆ Performance Metrics

All models are evaluated using:

- **F1-score (macro)** - PRIMARY METRIC
- **Accuracy**
- **Precision (macro)**
- **Recall (macro)**
- **5-fold stratified cross-validation**

### Computational Efficiency

- **Inference time** (ms per document)
- **Memory footprint** (MB)
- **Training time** (seconds)

### Expected Performance

A-LSA aims to achieve:
- **Competitive F1-scores** (within 1-2% of best model)
- **~40Ã— faster** than BERT
- **~9Ã— smaller** memory footprint than BERT

---

## ğŸ“Š Visualizations

The implementation generates publication-ready figures:

1. **Sensitivity to k**: F1-score vs latent dimension (k = 10-500)
2. **Impact of Imbalance**: Performance across imbalance ratios (1:1 to 1:10)
3. **t-SNE Visualization**: 2D projection of latent spaces
4. **Performance Comparison**: Bar plots comparing all models
5. **Characteristic Terms**: Top terms for each class

All figures are saved at **300 DPI** in PNG and PDF formats.

---

## ğŸ”¬ Baseline Models

For comparison, the following baselines are implemented:

| Model | Description |
|-------|-------------|
| **Naive Bayes** | Multinomial NB with Laplace smoothing |
| **Logistic Regression** | L2-regularized (C=1.0) |
| **Linear SVM** | Linear kernel (C=1.0) |
| **LSA + LR** | Classical LSA (k=100) + Logistic Regression |
| **BERT** (optional) | Fine-tuned BERT-base-uncased |

---

## ğŸ“– API Reference

### AdaptiveLSA

```python
class AdaptiveLSA(n_components=100, max_features=None,
                   min_df=2, max_df=0.95, random_state=None)
```

**Parameters:**
- `n_components`: Latent dimension k (default: 100)
- `max_features`: Maximum vocabulary size
- `min_df`: Minimum document frequency
- `max_df`: Maximum document frequency
- `random_state`: Random seed

**Methods:**
- `fit(X, y)`: Train the model
- `predict(X)`: Predict class labels
- `predict_proba(X)`: Predict class probabilities
- `decision_function(X)`: Get differential semantic distances
- `get_latent_projections(X)`: Get latent space projections
- `get_characteristic_terms(n_terms=10)`: Extract top terms per class

---

## ğŸ§ª Running Tests

```bash
# Install test dependencies
pip install pytest

# Run tests
pytest tests/
```

---

## ğŸ“„ Citation

If you use this implementation in your research, please cite:

```bibtex
@misc{touza2026alsa,
  title={Adaptive Latent Semantic Analysis for Binary Text Classification},
  author={Touza, Isaac},
  year={2026},
  institution={UniversitÃ© de Maroua, Cameroun}
}
```

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“§ Contact

**Isaac Touza**
UniversitÃ© de Maroua, Cameroun
Email: isaac.touza@univ-maroua.cm

---

## ğŸ™ Acknowledgments

- UCI Machine Learning Repository for SMS Spam dataset
- Stanford University for IMDb dataset
- Carnegie Mellon University for 20 Newsgroups dataset
- scikit-learn developers for excellent machine learning tools

---

## ğŸ“š References

### Datasets
1. Almeida, T.A., Hidalgo, J.M.G., Yamakami, A. (2011). *Contributions to the study of SMS spam filtering*
2. Maas, A.L., et al. (2011). *Learning word vectors for sentiment analysis*. ACL-HLT
3. Lang, K. (1995). *NewsWeeder: Learning to filter netnews*. ICML

### Methods
- Deerwester, S., et al. (1990). *Indexing by latent semantic analysis*. JASIS
- Golub, G.H., Van Loan, C.F. (2013). *Matrix Computations*. Johns Hopkins University Press

---

**Made with â¤ï¸ for the NLP research community**
