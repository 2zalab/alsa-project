{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# A-LSA Demonstration Notebook\n",
    "\n",
    "**Adaptive Latent Semantic Analysis for Binary Text Classification**\n",
    "\n",
    "Author: Isaac Touza  \n",
    "Institution: Université de Maroua, Cameroun  \n",
    "Date: January 2026\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates the complete A-LSA workflow across **three diverse datasets**:\n",
    "\n",
    "1. **20 Newsgroups** - Topic classification (comp.graphics vs rec.sport.hockey)\n",
    "2. **IMDb Movie Reviews** - Sentiment analysis (positive vs negative)\n",
    "3. **SMS Spam Collection** - Spam detection (spam vs ham)\n",
    "\n",
    "For each dataset, we will:\n",
    "- Load and preprocess the data\n",
    "- Train A-LSA and baseline models\n",
    "- Evaluate and compare performance\n",
    "- Visualize results and characteristic terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_20newsgroups\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Add parent directory to path\nsys.path.append('..')\n\nfrom src.alsa import AdaptiveLSA\nfrom src.baselines import get_baseline_models, create_imbalanced_dataset\nfrom src.evaluation import evaluate_model, compare_models\nfrom src.visualization import (\n    plot_tsne_visualization,\n    plot_characteristic_terms,\n    plot_performance_comparison,\n    plot_sensitivity_to_k,\n    plot_imbalance_impact\n)\n\n# Set visualization style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# Configuration\nRANDOM_STATE = 42\nTEST_SIZE = 0.2\nN_COMPONENTS = 100\n\nprint(\"✓ Imports successful\")"
  },
  {
   "cell_type": "markdown",
   "id": "dataset-loaders",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_20newsgroups_data():\n",
    "    \"\"\"Load 20 Newsgroups dataset.\"\"\"\n",
    "    categories = ('comp.graphics', 'rec.sport.hockey')\n",
    "    \n",
    "    data = fetch_20newsgroups(\n",
    "        subset='all',\n",
    "        categories=categories,\n",
    "        remove=('headers', 'footers', 'quotes'),\n",
    "        shuffle=True,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    return data.data, data.target, categories\n",
    "\n",
    "\n",
    "def load_imdb_data(data_dir='../data/imdb', max_samples=10000):\n",
    "    \"\"\"Load IMDb Movie Reviews dataset.\"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    if not os.path.exists(os.path.join(data_dir, 'train', 'pos')):\n",
    "        print(f\"⚠ IMDb dataset not found at {data_dir}\")\n",
    "        print(\"  Download from: https://ai.stanford.edu/~amaas/data/sentiment/\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Load data\n",
    "    for split in ['train', 'test']:\n",
    "        for label_name, label_value in [('pos', 1), ('neg', 0)]:\n",
    "            dir_path = os.path.join(data_dir, split, label_name)\n",
    "            \n",
    "            if not os.path.exists(dir_path):\n",
    "                continue\n",
    "            \n",
    "            for filename in os.listdir(dir_path):\n",
    "                if filename.endswith('.txt'):\n",
    "                    with open(os.path.join(dir_path, filename), 'r', encoding='utf-8') as f:\n",
    "                        texts.append(f.read())\n",
    "                        labels.append(label_value)\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Subsample for efficiency\n",
    "    if len(texts) > max_samples:\n",
    "        indices = np.random.RandomState(RANDOM_STATE).choice(\n",
    "            len(texts), max_samples, replace=False\n",
    "        )\n",
    "        texts = [texts[i] for i in indices]\n",
    "        labels = labels[indices]\n",
    "    \n",
    "    return texts, labels, ('Negative', 'Positive')\n",
    "\n",
    "\n",
    "def load_sms_spam_data(data_path='../data/sms_spam/SMSSpamCollection'):\n",
    "    \"\"\"Load SMS Spam Collection dataset.\"\"\"\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"⚠ SMS Spam dataset not found at {data_path}\")\n",
    "        print(\"  Download from: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\")\n",
    "        return None, None, None\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t', 1)\n",
    "            if len(parts) == 2:\n",
    "                label, text = parts\n",
    "                texts.append(text)\n",
    "                labels.append(1 if label == 'spam' else 0)\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return texts, labels, ('Ham', 'Spam')\n",
    "\n",
    "\n",
    "print(\"✓ Dataset loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: 20 Newsgroups Dataset\n",
    "\n",
    "Topic classification between comp.graphics and rec.sport.hockey\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ng-load",
   "metadata": {},
   "source": [
    "## 3.1 Load 20 Newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-ng",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading 20 Newsgroups dataset...\")\n",
    "X_ng, y_ng, categories_ng = load_20newsgroups_data()\n",
    "\n",
    "print(f\"✓ Loaded {len(X_ng)} documents\")\n",
    "print(f\"  - {categories_ng[0]}: {np.sum(y_ng == 0)} documents ({100*np.mean(y_ng == 0):.1f}%)\")\n",
    "print(f\"  - {categories_ng[1]}: {np.sum(y_ng == 1)} documents ({100*np.mean(y_ng == 1):.1f}%)\")\n",
    "\n",
    "# Split data\n",
    "X_ng_train, X_ng_test, y_ng_train, y_ng_test = train_test_split(\n",
    "    X_ng, y_ng, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_ng\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining: {len(X_ng_train)} | Test: {len(X_ng_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ng-train",
   "metadata": {},
   "source": [
    "## 3.2 Train Models on 20 Newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-ng",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize models with optimized A-LSA parameters\nprint(\"Initializing models...\")\nmodels_ng = {}\nmodels_ng['A-LSA'] = AdaptiveLSA(\n    n_components=N_COMPONENTS,\n    normalize_energies=True,\n    optimize_threshold=True,\n    random_state=RANDOM_STATE\n)\nmodels_ng.update(get_baseline_models(n_components=N_COMPONENTS, random_state=RANDOM_STATE))\n\n# Train and evaluate\nprint(\"\\nTraining and evaluating models...\")\nresults_ng = compare_models(\n    models=models_ng,\n    X_train=X_ng_train,\n    y_train=y_ng_train,\n    X_test=X_ng_test,\n    y_test=y_ng_test,\n    n_cv_folds=5,\n    random_state=RANDOM_STATE\n)\n\nresults_ng['Dataset'] = '20 Newsgroups'\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"20 NEWSGROUPS RESULTS\")\nprint(\"=\"*80)\nprint(results_ng[['Model', 'Test F1 (macro)', 'Test Accuracy']].to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "id": "ng-viz",
   "metadata": {},
   "source": [
    "## 3.3 Visualize 20 Newsgroups Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-ng",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characteristic terms\n",
    "alsa_ng = models_ng['A-LSA']\n",
    "char_terms_ng = alsa_ng.get_characteristic_terms(n_terms=10)\n",
    "\n",
    "plot_characteristic_terms(\n",
    "    terms_pos=char_terms_ng['positive'],\n",
    "    terms_neg=char_terms_ng['negative'],\n",
    "    class_names=categories_ng,\n",
    "    n_terms=10\n",
    ")\n",
    "\n",
    "print(f\"\\nTop {categories_ng[1]} terms:\", [t for t, _ in char_terms_ng['positive'][:5]])\n",
    "print(f\"Top {categories_ng[0]} terms:\", [t for t, _ in char_terms_ng['negative'][:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tsne-ng",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization\n",
    "z_pos_ng, z_neg_ng = alsa_ng.get_latent_projections(X_ng_test)\n",
    "\n",
    "plot_tsne_visualization(\n",
    "    z_pos=z_pos_ng,\n",
    "    z_neg=z_neg_ng,\n",
    "    y_true=y_ng_test,\n",
    "    class_names=categories_ng,\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: IMDb Movie Reviews\n",
    "\n",
    "Sentiment analysis (positive vs negative reviews)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imdb-load",
   "metadata": {},
   "source": [
    "## 4.1 Load IMDb Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-imdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading IMDb dataset...\")\n",
    "X_imdb, y_imdb, categories_imdb = load_imdb_data()\n",
    "\n",
    "if X_imdb is not None:\n",
    "    print(f\"✓ Loaded {len(X_imdb)} reviews\")\n",
    "    print(f\"  - {categories_imdb[0]}: {np.sum(y_imdb == 0)} reviews ({100*np.mean(y_imdb == 0):.1f}%)\")\n",
    "    print(f\"  - {categories_imdb[1]}: {np.sum(y_imdb == 1)} reviews ({100*np.mean(y_imdb == 1):.1f}%)\")\n",
    "    \n",
    "    # Split data\n",
    "    X_imdb_train, X_imdb_test, y_imdb_train, y_imdb_test = train_test_split(\n",
    "        X_imdb, y_imdb, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_imdb\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining: {len(X_imdb_train)} | Test: {len(X_imdb_test)}\")\n",
    "else:\n",
    "    print(\"⚠ Skipping IMDb dataset (not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imdb-train",
   "metadata": {},
   "source": [
    "## 4.2 Train Models on IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-imdb",
   "metadata": {},
   "outputs": [],
   "source": "if X_imdb is not None:\n    # Initialize models with optimized A-LSA parameters\n    print(\"Initializing models...\")\n    models_imdb = {}\n    models_imdb['A-LSA'] = AdaptiveLSA(\n        n_components=N_COMPONENTS,\n        normalize_energies=True,\n        optimize_threshold=True,\n        random_state=RANDOM_STATE\n    )\n    models_imdb.update(get_baseline_models(n_components=N_COMPONENTS, random_state=RANDOM_STATE))\n    \n    # Train and evaluate\n    print(\"\\nTraining and evaluating models...\")\n    results_imdb = compare_models(\n        models=models_imdb,\n        X_train=X_imdb_train,\n        y_train=y_imdb_train,\n        X_test=X_imdb_test,\n        y_test=y_imdb_test,\n        n_cv_folds=5,\n        random_state=RANDOM_STATE\n    )\n    \n    results_imdb['Dataset'] = 'IMDb'\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"IMDB RESULTS\")\n    print(\"=\"*80)\n    print(results_imdb[['Model', 'Test F1 (macro)', 'Test Accuracy']].to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "id": "imdb-viz",
   "metadata": {},
   "source": [
    "## 4.3 Visualize IMDb Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-imdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_imdb is not None:\n",
    "    # Characteristic terms\n",
    "    alsa_imdb = models_imdb['A-LSA']\n",
    "    char_terms_imdb = alsa_imdb.get_characteristic_terms(n_terms=10)\n",
    "    \n",
    "    plot_characteristic_terms(\n",
    "        terms_pos=char_terms_imdb['positive'],\n",
    "        terms_neg=char_terms_imdb['negative'],\n",
    "        class_names=categories_imdb,\n",
    "        n_terms=10\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTop {categories_imdb[1]} sentiment terms:\", [t for t, _ in char_terms_imdb['positive'][:5]])\n",
    "    print(f\"Top {categories_imdb[0]} sentiment terms:\", [t for t, _ in char_terms_imdb['negative'][:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: SMS Spam Collection\n",
    "\n",
    "Spam detection (spam vs ham messages)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sms-load",
   "metadata": {},
   "source": [
    "## 5.1 Load SMS Spam Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-sms",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading SMS Spam dataset...\")\n",
    "X_sms, y_sms, categories_sms = load_sms_spam_data()\n",
    "\n",
    "if X_sms is not None:\n",
    "    print(f\"✓ Loaded {len(X_sms)} messages\")\n",
    "    print(f\"  - {categories_sms[0]}: {np.sum(y_sms == 0)} messages ({100*np.mean(y_sms == 0):.1f}%)\")\n",
    "    print(f\"  - {categories_sms[1]}: {np.sum(y_sms == 1)} messages ({100*np.mean(y_sms == 1):.1f}%)\")\n",
    "    \n",
    "    # Split data\n",
    "    X_sms_train, X_sms_test, y_sms_train, y_sms_test = train_test_split(\n",
    "        X_sms, y_sms, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_sms\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining: {len(X_sms_train)} | Test: {len(X_sms_test)}\")\n",
    "else:\n",
    "    print(\"⚠ Skipping SMS Spam dataset (not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sms-train",
   "metadata": {},
   "source": [
    "## 5.2 Train Models on SMS Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-sms",
   "metadata": {},
   "outputs": [],
   "source": "if X_sms is not None:\n    # Initialize models with optimized A-LSA parameters for SMS (short texts)\n    print(\"Initializing models...\")\n    models_sms = {}\n    models_sms['A-LSA'] = AdaptiveLSA(\n        n_components=75,  # Optimal for SMS Spam\n        min_df=1,  # Lower min_df for short texts\n        normalize_energies=True,\n        optimize_threshold=True,\n        random_state=RANDOM_STATE\n    )\n    models_sms.update(get_baseline_models(n_components=N_COMPONENTS, random_state=RANDOM_STATE))\n    \n    # Train and evaluate\n    print(\"\\nTraining and evaluating models...\")\n    results_sms = compare_models(\n        models=models_sms,\n        X_train=X_sms_train,\n        y_train=y_sms_train,\n        X_test=X_sms_test,\n        y_test=y_sms_test,\n        n_cv_folds=5,\n        random_state=RANDOM_STATE\n    )\n    \n    results_sms['Dataset'] = 'SMS Spam'\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"SMS SPAM RESULTS\")\n    print(\"=\"*80)\n    print(results_sms[['Model', 'Test F1 (macro)', 'Test Accuracy']].to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "id": "sms-viz",
   "metadata": {},
   "source": [
    "## 5.3 Visualize SMS Spam Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-sms",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_sms is not None:\n",
    "    # Characteristic terms\n",
    "    alsa_sms = models_sms['A-LSA']\n",
    "    char_terms_sms = alsa_sms.get_characteristic_terms(n_terms=10)\n",
    "    \n",
    "    plot_characteristic_terms(\n",
    "        terms_pos=char_terms_sms['positive'],\n",
    "        terms_neg=char_terms_sms['negative'],\n",
    "        class_names=categories_sms,\n",
    "        n_terms=10\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTop {categories_sms[1]} terms:\", [t for t, _ in char_terms_sms['positive'][:5]])\n",
    "    print(f\"Top {categories_sms[0]} terms:\", [t for t, _ in char_terms_sms['negative'][:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Cross-Dataset Comparison\n",
    "\n",
    "Compare A-LSA performance across all three datasets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "## 6. Compare Results Across Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = [results_ng]\n",
    "\n",
    "if X_imdb is not None:\n",
    "    all_results.append(results_imdb)\n",
    "    \n",
    "if X_sms is not None:\n",
    "    all_results.append(results_sms)\n",
    "\n",
    "combined_results = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CROSS-DATASET COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(combined_results[['Dataset', 'Model', 'Test F1 (macro)', 'Test Accuracy']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize A-LSA performance across datasets\n",
    "alsa_results = combined_results[combined_results['Model'] == 'A-LSA'].copy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# F1 scores\n",
    "axes[0].bar(alsa_results['Dataset'], alsa_results['Test F1 (macro)'], color='steelblue', alpha=0.7)\n",
    "axes[0].set_ylabel('F1 Score (macro)', fontweight='bold')\n",
    "axes[0].set_title('A-LSA F1 Performance Across Datasets', fontweight='bold')\n",
    "axes[0].set_ylim([0.7, 1.0])\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Accuracy\n",
    "axes[1].bar(alsa_results['Dataset'], alsa_results['Test Accuracy'], color='coral', alpha=0.7)\n",
    "axes[1].set_ylabel('Accuracy', fontweight='bold')\n",
    "axes[1].set_title('A-LSA Accuracy Across Datasets', fontweight='bold')\n",
    "axes[1].set_ylim([0.7, 1.0])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrated A-LSA across three diverse datasets:\n\n### 20 Newsgroups\n- ✓ Topic classification task\n- ✓ Balanced dataset\n- ✓ Medium-length documents\n\n### IMDb Movie Reviews\n- ✓ Sentiment analysis task\n- ✓ Balanced dataset\n- ✓ Long documents (~230 words)\n\n### SMS Spam Collection\n- ✓ Spam detection task\n- ✓ Imbalanced dataset (13% spam)\n- ✓ Very short texts (~80 characters)\n- ✓ **Optimal performance with k=75, min_df=1**\n\n**Key Findings:**\n- A-LSA achieves competitive or superior performance across all datasets\n- Dual latent spaces effectively capture class-specific semantics\n- Method is robust to varying text lengths and class imbalance\n- Characteristic terms provide interpretable insights\n- **Energy normalization and threshold optimization are critical for imbalanced data**\n- **Performance is relatively stable across k values (50-150)**\n\n**Recent Improvements:**\n- ✓ Energy normalization by explained variance ratio\n- ✓ Validation-based threshold optimization\n- ✓ Adaptive parameters for different text lengths\n- ✓ Robust to class imbalance (1:10 ratio)"
  },
  {
   "cell_type": "code",
   "id": "dvjz79fhst",
   "source": "# Test robustness to imbalance on 20 Newsgroups dataset\nprint(\"Testing robustness to class imbalance on 20 Newsgroups...\")\n\nimbalance_ratios = [1.0, 0.5, 0.33, 0.2, 0.1]  # 1:1 to 1:10\nf1_scores_imb = {\n    'A-LSA': [],\n    'A-LSA (no θ adjustment)': [],\n    'Logistic Regression': []\n}\n\nfor ratio in imbalance_ratios:\n    print(f\"\\nTesting ratio {ratio:.2f} (1:{int(1/ratio)})...\")\n    \n    # Create imbalanced training set\n    if ratio < 1.0:\n        X_train_imb, y_train_imb = create_imbalanced_dataset(\n            X_ng_train,\n            y_ng_train,\n            ratio=ratio,\n            random_state=RANDOM_STATE\n        )\n    else:\n        X_train_imb = X_ng_train\n        y_train_imb = y_ng_train\n    \n    print(f\"  Training set: {len(X_train_imb)} samples\")\n    print(f\"  Positive: {np.sum(y_train_imb == 1)}, Negative: {np.sum(y_train_imb == 0)}\")\n    \n    # A-LSA with threshold optimization\n    alsa_opt = AdaptiveLSA(\n        n_components=N_COMPONENTS,\n        normalize_energies=True,\n        optimize_threshold=True,\n        random_state=RANDOM_STATE\n    )\n    alsa_opt.fit(X_train_imb, y_train_imb)\n    alsa_opt_metrics = evaluate_model(alsa_opt, X_ng_test, y_ng_test)\n    f1_scores_imb['A-LSA'].append(alsa_opt_metrics['F1-score (macro)'])\n    print(f\"  A-LSA F1: {alsa_opt_metrics['F1-score (macro)']:.4f} (θ={alsa_opt.theta_:.4f})\")\n    \n    # A-LSA without threshold optimization\n    alsa_no_opt = AdaptiveLSA(\n        n_components=N_COMPONENTS,\n        normalize_energies=True,\n        optimize_threshold=False,\n        random_state=RANDOM_STATE\n    )\n    alsa_no_opt.fit(X_train_imb, y_train_imb)\n    alsa_no_opt.theta_ = 0.0  # Force θ = 0\n    alsa_no_opt_metrics = evaluate_model(alsa_no_opt, X_ng_test, y_ng_test)\n    f1_scores_imb['A-LSA (no θ adjustment)'].append(alsa_no_opt_metrics['F1-score (macro)'])\n    print(f\"  A-LSA (no θ) F1: {alsa_no_opt_metrics['F1-score (macro)']:.4f}\")\n    \n    # Logistic Regression baseline\n    lr_models = get_baseline_models(random_state=RANDOM_STATE)\n    lr = lr_models['Logistic Regression']\n    lr.fit(X_train_imb, y_train_imb)\n    lr_metrics = evaluate_model(lr, X_ng_test, y_ng_test)\n    f1_scores_imb['Logistic Regression'].append(lr_metrics['F1-score (macro)'])\n    print(f\"  LR F1: {lr_metrics['F1-score (macro)']:.4f}\")\n\n# Plot results\nplot_imbalance_impact(\n    imbalance_ratios=imbalance_ratios,\n    f1_scores=f1_scores_imb\n)\n\nprint(\"\\n✓ Imbalance analysis complete\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3j1t82g5hbs",
   "source": "## 7.2 Robustness to Class Imbalance\n\nHow well does A-LSA handle varying degrees of class imbalance?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "jjx4jl1b0nc",
   "source": "# Test sensitivity to k on 20 Newsgroups dataset\nprint(\"Testing sensitivity to k on 20 Newsgroups...\")\n\nk_values = [10, 25, 50, 75, 100, 125, 150, 200]\nf1_scores_k = {\n    'A-LSA': [],\n    'LSA + Logistic Regression': []\n}\n\nfor k in k_values:\n    print(f\"\\nTesting k={k}...\")\n    \n    # A-LSA\n    alsa_k = AdaptiveLSA(\n        n_components=k,\n        normalize_energies=True,\n        optimize_threshold=True,\n        random_state=RANDOM_STATE\n    )\n    alsa_k.fit(X_ng_train, y_ng_train)\n    alsa_metrics = evaluate_model(alsa_k, X_ng_test, y_ng_test)\n    f1_scores_k['A-LSA'].append(alsa_metrics['F1-score (macro)'])\n    print(f\"  A-LSA F1: {alsa_metrics['F1-score (macro)']:.4f}\")\n    \n    # LSA + LR\n    lsa_lr_models = get_baseline_models(n_components=k, random_state=RANDOM_STATE)\n    lsa_lr = lsa_lr_models['LSA + Logistic Regression']\n    lsa_lr.fit(X_ng_train, y_ng_train)\n    lsa_lr_metrics = evaluate_model(lsa_lr, X_ng_test, y_ng_test)\n    f1_scores_k['LSA + Logistic Regression'].append(lsa_lr_metrics['F1-score (macro)'])\n    print(f\"  LSA+LR F1: {lsa_lr_metrics['F1-score (macro)']:.4f}\")\n\n# Plot results\nplot_sensitivity_to_k(\n    k_values=k_values,\n    f1_scores=f1_scores_k,\n    optimal_k=100\n)\n\nprint(\"\\n✓ Sensitivity analysis complete\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3yr9yihiae",
   "source": "## 7.1 Sensitivity to Latent Dimension k\n\nHow does performance vary with the dimensionality of the latent space?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "ks4caaeft7",
   "source": "---\n\n# Part 5: Sensitivity Analysis\n\nAnalyze A-LSA robustness to hyperparameters and data conditions\n\n---",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}